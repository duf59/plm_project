---
title: "Practical Machine Learning Course Project"
author: "Renaud Dufour"
date: "Wednesday, May 20, 2015"
output: html_document
---

```{r setup}
library(caret)
```

# to do

* Select model with few predictors and write repport.

# Read the data

```{r}
data <- read.csv("./source_data/pml-training.csv", na.string = c(NA,""))
validation  <- read.csv("./source_data/pml-testing.csv", na.string = c(NA,""))

```

# preprocessing

Looking at the summary we se that 100 variables out of 160 have 98% missing values in the data set.
Same variables have 100% missing values in the test set. We get rid of them.

```{r}
idx1 <- which(sapply(data,function(x) sum(is.na(x))/nrow(data))>0.95)
data <- data[,-idx1]
```

We now have 60 variables.
We remove the X variable corresponding to the row number.
Besides, purpose being to predict the classe from the sensors informations,
it is not relevant to use time and date informations (this even if they could appear to be significant predictors).
We however keep the num_window

```{r}
data <- subset(data, select=-c(X,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp,new_window))
```

This brings the number of variables down to 55.
See if some of them have near zero variance :

```{r}
nsv <- nearZeroVar(data,saveMetrics=TRUE)
nsv
```

No variable appears to have near zero variance.
Look at correlation between variables.

```{r}
corMatrix <- abs(cor(subset(data, select=-c(user_name,classe))))>.8
diag(corMatrix) <- FALSE # set autocorrelation coefficients to FALSE
colSums(corMatrix) # USE THIS to index and display

corMatrix[colSums(corMatrix)!=0,colSums(corMatrix)!=0] # display only submatrix with relevant correlations
```

For threshold of 0.8 we have the following strong correlation:

* total_accel_belt with accel_belt_y and accel_belt_z (last 2 also correlated) - keep 1 of them
* magnet_belt_x with accel_belt_x and pitch_belt (last 2 also correlated) - keep 1 (pitch belt?)
* gyro_arm_x with gyro_arm_y
* magnet_arm_x with accel_arm_x
* magnet_arm_y with magnet_arm_z
* pitch_dumbbell with accel_dumbbell_x
* yaw_dumbdell with accel_dumbbell_z
* gyros_forearm_z corr with gyros_forearm_y and gyros_dumbbell_z and gyros_dumbbell_x (last 2 also correlated)

# Split into training and test sets

```{r split}
set.seed(3764)
inTrain<- createDataPartition(data$classe, p = 0.75, list = FALSE)
training <- data[inTrain,]
testing  <- data[-inTrain,]
nrow(training)
nrow(testing)
table(training$classe)
table(testing$classe)
```

# Run basic models with all variables (55 predictors)

## LDA model

```{r lda}
ldafit <- train(classe ~ ., data=train, method = "lda", preProcess = c("center","scale"))
```

## Random Forest model

* Train a random forest on the training set considering all predictors with 10-fold cross-validation.

```{r randomForest fit}
numFolds = trainControl( method = "cv", number = 10)
cpGrid = expand.grid( .cp = seq(0.001,0.05,by = 0.001)) 

t1 <- Sys.time()
rffit <- train(classe ~ ., method="rf", data=training, trControl = numFolds)
t2 <- Sys.time()
t2-t1 # takes about 30min for 55 predictors and 10-fold CV

saveRDS(rffit, "rfmodelfull.rds")
rffit <- readRDS(file = "rfmodelfull.rds")

rffit

plot(varImp(rffit))
```

The best RF fit is obtained for mtry=30, leading to a CV accuracy of 0.997.
From the Variable Importance plot we note that :

* num_window is the most important, although it may not be ideal to rely on such variable
* there is an elbow after the first 8 most important variables
* User names are not relevant
* gyros... variables are not or weekly relevant
* but this is to contrast with the correlated variables

We then evaluate this model on the testing set :

```{r randomForest evaluation}
testPred <- predict(rffit, newdata = testing)
confusionMatrix(testPred,testing$classe)
```

We achieve an accuracy of 0.9978 on the hold out training set.
This confirm we are not overfitting and the model is accurate.

Predict on the validation set

```{r randomForest final prediction}
testPred <- predict(rffit, newdata = validation)
```

# Decreasing the number of predictors

We previously used all 55 predictors. He we investigate if we can achieve a more interpretable model
based on a small number of predictors.

## Random forest without the num_window predictor

```{r randomForest fit no num_window}
numFolds = trainControl( method = "cv", number = 10)
cpGrid = expand.grid( .cp = seq(0.001,0.05,by = 0.001)) 

t1 <- Sys.time()
rffit2 <- train(classe ~ ., method="rf", data=training, trControl = numFolds)
t2 <- Sys.time()
t2-t1

saveRDS(rffit2, "rfmodel-no-num_window.rds")
rffit2

plot(varImp(rffit2))
```

Removing this predictor only decrease the CV predicted accuracy by a small amount : to 0.993 from 0.997

```{r randomForest evaluation2}
testPred <- predict(rffit2, newdata = testing)
confusionMatrix(testPred,testing$classe)
```

Accuracy on the held out set is consistent with 0.993

## Random forest without the num_window predictor and without user_names

From variable importance measure we have seen that user_names are not often selecten for splitting the trees.
We see how the expected accuracy changes by removing this variable.

```{r randomForest fit no num_window no user}
numFolds = trainControl( method = "cv", number = 10)
cpGrid = expand.grid( .cp = seq(0.001,0.05,by = 0.001)) 

t1 <- Sys.time()
rffit3 <- train(classe ~ . -num_window - user_name, method="rf", data=training, trControl = numFolds)
t2 <- Sys.time()
t2-t1

saveRDS(rffit3, "rfmodel-no-num_window-no-user_name.rds")
rffit3 # approx 33min

plot(varImp(rffit3))
```

Removing user_name predictor in addition to num_windows decreases the CV accuracy from 0.997 for the full dataset to 0.993

```{r randomForest evaluation2}
testPred <- predict(rffit3, newdata = testing)
confusionMatrix(testPred,testing$classe)
```

Accuracy on the held out set is consistent with 0.992

## Removing more variables

Lets now decrease the number of predictor by removing variables having low importance and being strongly correlated with other variables (see correlation analysis above). We remove the following variables :

```{r restricted dataset}
training2 <- subset(training, select=-c(num_window,user_name,
                                        gyros_forearm_x,gyros_forearm_y,gyros_forearm_z,
                                        gyros_arm_x,gyros_arm_y,gyros_arm_z,
                                        gyros_dumbbell_x,gyros_dumbbell_y,gyros_dumbbell_z,
                                        gyros_belt_x,gyros_belt_y,gyros_belt_z,
                                        yaw_dumbbell, pitch_dumbbell,
                                        magnet_arm_z,
                                        accel_arm_x, accel_arm_y, accel_arm_z,
                                        total_accel_forearm, total_accel_arm, total_accel_belt,
                                        accel_belt_x,accel_belt_y,accel_belt_z))

```


```{r randomForest fit no num_window no user}
numFolds = trainControl( method = "cv", number = 10)
cpGrid = expand.grid( .cp = seq(0.001,0.05,by = 0.001)) 

t1 <- Sys.time()
rffit4 <- train(classe ~ ., method="rf", data=training2, trControl = numFolds)
t2 <- Sys.time()
t2-t1 # about 15min

saveRDS(rffit4, "rfmodel-training2.rds")
rffit4

plot(varImp(rffit4))
```

Removing user_name predictor in addition to num_windows decreases the CV accuracy from 0.997 for the full dataset to 0.992

```{r randomForest evaluation2}
testPred <- predict(rffit4, newdata = testing)
confusionMatrix(testPred,testing$classe)
```

Accuracy on the held out set is consistent with 0.993

## Removing even more variables

Continue selecting variables iteratively, keeping accuracy above 99%

```{r restricted dataset}
training3 <- subset(training, select=-c(user_name,
                                        gyros_forearm_x,gyros_forearm_y,gyros_forearm_z,
                                        gyros_arm_x,gyros_arm_y,gyros_arm_z,
                                        gyros_dumbbell_x,gyros_dumbbell_y,gyros_dumbbell_z,
                                        gyros_belt_x,gyros_belt_y,gyros_belt_z,
                                        yaw_dumbbell, pitch_dumbbell,
                                        magnet_arm_x, magnet_arm_y, magnet_arm_z,
                                        pitch_arm,
                                        yaw_forearm,
                                        accel_arm_x, accel_arm_y, accel_arm_z,
                                        total_accel_forearm, total_accel_arm, total_accel_belt, total_accel_dumbbell,
                                        accel_belt_x,accel_belt_y,accel_belt_z,
                                        accel_forearm_x,accel_forearm_y,accel_forearm_z,
                                        magnet_forearm_x,magnet_forearm_y,magnet_forearm_z,
                                        accel_dumbbell_x, accel_dumbbell_y, accel_dumbbell_z,
                                        magnet_belt_x, magnet_belt_y, magnet_belt_z,
                                        yaw_arm, roll_arm))

```


```{r randomForest fit no num_window no user}
numFolds = trainControl( method = "cv", number = 10)
# cpGrid = expand.grid( .cp = seq(0.001,0.05,by = 0.001))
cpGrid = expand.grid( .mtry = seq(3,10,by = 1))

t1 <- Sys.time()
rffit5 <- train(classe ~ ., method="rf", data=training3, trControl = numFolds, tuneGrid = cpGrid)
t2 <- Sys.time()
t2-t1

saveRDS(rffit5, "rfmodel-training3.rds")
rffit5

plot(varImp(rffit5))
```

Removing user_name predictor in addition to num_windows decreases the CV accuracy from 0.997 for the full dataset to 0.992

```{r randomForest evaluation2}
testPred <- predict(rffit5, newdata = testing)
confusionMatrix(testPred,testing$classe)
```

predict on validation set :

```{r randomForest evaluation2}
testPred <- predict(rffit5, newdata = validation)
confusionMatrix(testPred,testing$classe)
```

Accuracy on the held out set is consistent with 0.993

# CART model

```{r CART fit}
numFolds = trainControl( method = "cv", number = 10)
cpGrid = expand.grid( .cp = seq(1e-8,1e-4,length.out = 100)) 

t1 <- Sys.time()
CARTfit <- train(classe ~ ., method="rpart", data=training2, trControl = numFolds, tuneGrid = cpGrid)

t2 <- Sys.time()
t2-t1

saveRDS(CARTfit, "rfmodel-training3.rds")
CARTfit

plot(varImp(CARTfit))
```

```{r CART evaluation2}
testPred <- predict(CARTfit, newdata = testing)
confusionMatrix(testPred,testing$classe)
```


```{r CART final evaluation}
testPredCART <- predict(CARTfit, newdata = validation)
confusionMatrix(testPred,testPredCART)
```